# -*- coding: utf-8 -*-
"""HQ-CNN-SA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I67PFuFcU0go2nub06v7TJNQWbtgYv79

IMPORTING QUANTUM LIBRARIES  AND TENSORFLOW
"""

!pip install pennylane --upgrade
from IPython.display import clear_output
clear_output(wait=False)

import tensorflow as tf
from pennylane import numpy as np
from tensorflow import keras
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, Model

"""***GETTING Q INPUT ***"""

x_train_final = np.load("/content/drive/MyDrive/NUMPY FILE/x_train_final3.npy")
y_train = np.load("/content/drive/MyDrive/NUMPY FILE/y_train3.npy")
x_test_final = np.load("/content/drive/MyDrive/NUMPY FILE/x_test_final3.npy")
y_test = np.load("/content/drive/MyDrive/NUMPY FILE/y_test3.npy")

x_train_final.shape,y_train.shape,x_test_final.shape,y_test.shape

"""***NORMALIZATION ***"""

def normalize(image):
    return 2 * ((image - tf.reduce_min(image)) / (tf.reduce_max(image) - tf.reduce_min(image))) - 1

import tensorflow as tf
def normalize(image):
    return (image - tf.reduce_min(image)) / (tf.reduce_max(image) - tf.reduce_min(image))
x_train_normalized = normalize(x_train_final)
x_test_normalized = normalize(x_test_final)

"""***HQ_CNN_SA***"""

import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report, cohen_kappa_score, matthews_corrcoef
from sklearn.utils.class_weight import compute_class_weight
import numpy as np
import time
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.saving import register_keras_serializable

# Register the SpatialAttention class
@register_keras_serializable()
class SpatialAttention(layers.Layer):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        self.conv = layers.Conv2D(1, kernel_size, padding='same', activation='sigmoid', kernel_initializer='he_normal')

    def call(self, inputs):
        avg_out = tf.reduce_mean(inputs, axis=-1, keepdims=True)
        max_out = tf.reduce_max(inputs, axis=-1, keepdims=True)
        concat = tf.concat([avg_out, max_out], axis=-1)
        return inputs * self.conv(concat)

# Define improved CNN model
def improved_cnn_model_with_attention(input_shape):
    inputs = layers.Input(shape=input_shape)
    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)

    x = SpatialAttention()(x)
    x = SpatialAttention()(x)

    x = layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)

    x = layers.Flatten()(x)
    x = layers.Dropout(0.5)(x)
    x = layers.Dense(1024, activation='relu')(x)
    x = layers.Dense(1024, activation='relu')(x)
    outputs = layers.Dense(6, activation='softmax')(x)

    return models.Model(inputs, outputs)

# Define input shape
input_shape = (14, 14, 4)

# Compute class weights
classes = np.unique(y_train)
class_weights = compute_class_weight(class_weight="balanced", classes=classes, y=y_train)
class_weights = dict(enumerate(class_weights))
print("Class Weights:", class_weights)

# Start timing model creation
start_time = time.time()

model = improved_cnn_model_with_attention(input_shape)
opt = optimizers.Adagrad(learning_rate=0.04)
model.compile(optimizer=opt, loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.summary()

end_time = time.time()
print(f"Model creation and compilation took {end_time - start_time:.2f} seconds")

# Data Augmentation
train_datagen = ImageDataGenerator(
    rotation_range=5,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Start training
start_time = time.time()

history = model.fit(
    train_datagen.flow(x_train_final, y_train, batch_size=8),
    epochs=80,
    validation_data=(x_test_final, y_test),
    class_weight=class_weights,
    verbose=1
)

end_time = time.time()
print(f"Model training took {end_time - start_time:.2f} seconds")

# Prediction
start_time = time.time()
y_pred1 = model.predict(x_test_final)
y_pred_classes1 = np.argmax(y_pred1, axis=1)
end_time = time.time()
print(f"Model prediction took {end_time - start_time:.2f} seconds")

# Accuracy Stats
train_accuracies = history.history['accuracy']
val_accuracies = history.history['val_accuracy']
train_losses = history.history['loss']
val_losses = history.history['val_loss']

avg_train_acc = np.mean(train_accuracies)
avg_val_acc = np.mean(val_accuracies)

print(f"Average Training Accuracy: {avg_train_acc * 100:.2f}%")
print(f"Average Validation Accuracy: {avg_val_acc * 100:.2f}%")

# Metrics
y_true = y_test
precision = precision_score(y_true, y_pred_classes1, average='weighted')
recall = recall_score(y_true, y_pred_classes1, average='weighted')
f1 = f1_score(y_true, y_pred_classes1, average='weighted')
conf_matrix = confusion_matrix(y_true, y_pred_classes1)
tn = conf_matrix.sum() - (conf_matrix.sum(axis=1) + conf_matrix.sum(axis=0) - conf_matrix.diagonal())
fp = conf_matrix.sum(axis=0) - conf_matrix.diagonal()
specificity = tn / (tn + fp)
specificity_macro = specificity.mean()
kappa = cohen_kappa_score(y_true, y_pred_classes1)
mcc = matthews_corrcoef(y_true, y_pred_classes1)
report = classification_report(y_true, y_pred_classes1)

print("Precision :", precision)
print("Recall :", recall)
print("Specificity:", specificity_macro)
print("F1 Score :", f1)
print("Cohen's Kappa:", kappa)
print("Matthews Correlation Coefficient:", mcc)
print("Classification Report:\n", report)

# Plot training history
plt.figure(figsize=(12, 5))

# Accuracy plot
plt.subplot(1, 2, 1)
plt.plot(train_accuracies, label='Train Accuracy', marker='o')
plt.plot(val_accuracies, label='Val Accuracy', marker='s')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# Loss plot
plt.subplot(1, 2, 2)
plt.plot(train_losses, label='Train Loss', marker='o')
plt.plot(val_losses, label='Val Loss', marker='s')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()